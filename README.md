#### Запуск ETL вручну

Виконайте наступні команди:

```bash
pip install -r requirements.txt
python -m etl.run_etl
```

У результаті цих команд буде створений або перезаписаний файл `agg_result.db`.
Буде створена або перезаписана таблиця `agg_trades_weekly`, що містить трансформовані та агреговані дані з вхідного CSV‑файла.

#### Генерування звітності

Для генерування звітності, в свою чергу, треба виконати команду:

```bash
python -m reporting.run_reporting
```

Цей модуль бере дані з `agg_result.db`, додатково фільтрує та агрегує їх по користувачах.
В результаті буде створено два CSV‑файли, що містять дані по топ‑3 bronze‑клієнтах з найбільшим `total_volume` або `total_pnl` відповідно.
Також буде побудований графік розподілення `total_volume` по тижнях та типах користувачів.

#### CI/CD (GitHub Actions)

У проєкті налаштований CI‑workflow, який при `push` в гілку `main` або `workflow_dispatch` виконує job `etl`.
При його виконанні відбувається:  
 
  Чекаут репозиторію  
  Встановлення необхідної версії Python  
  Встановлення залежностей:
  ```bash
  python -m pip install --upgrade pip
  python -m pip install -r requirements.txt
  ```
  Запуск ETL‑workflow:
  ```bash
  python -m etl.run_etl
  ```
Таким чином, при цих тригерах автоматично виконується ETL‑пайплайн та перестворюється база.

#### Масштабування під 100+ млн рядків

Цей проєкт створений під невеликий обсяг даних. Для його масштабування можна розглянути наступні опції:

##### Реалізація ETL 
 
Необхідно впровадити інструменти оркестрації даних, такі як Airflow чи Prefect, які дозволяють:
- Керувати DAG‑ами
- Виконувати повторні запуски при падінні
- Реалізувати моніторинг задач та алерти
- Планувати розклад виконання

#####  Збереження та обробка даних 
 
Необхідно замінити локальну базу SQLite на сховище, яке буде легко масштабуватися, таке як ClickHouse, BigQuery, Snowflake тощо.
Це надасть наступні переваги:
- Можливість роботи з мільйонами рядків 
- Швидке виконання аналітичних запитів 
- Масштабування за об'ємом та навантаженням

Заміна batch на streaming (опційно):  
В залежності від бізнес‑потреб, якщо потрібна обробка майже в реальному часі, варто розглянути стрімінгову систему:  
- Kafka або Pulsar для відслідковування подій 
- Spark Structured Streaming або Flink для потокової агрегації. 

Перехід від pandas на більш адаптовані фреймворки, такі як PySpark, що дозволить:
- Виконувати задачі на кластері
- Адаптуватися під стрімінг

##### Моніторинг та перевірка якості даних

Для масштабованого проекту необхідно впровадити наступний моніторинг за допомогою вбудованих метрик або додаткових утиліт:
- Час виконання кожного модуля
- Підрахунок кількості оброблених рядків
- Час виконання запитів
- Кількість збоїв та повторних запусків

Задля перевірки якості даних треба відслідковувати:
- Кількість знайдених дублікатів
- Кількість пропущених значень 
- Відповідність параметрам валідацій різних полів
- Контроль обсягу: кількість рядків за період у межах очікуваного діапазону

##### Звітність та візуалізація даних

Для кращої звітності варто використати BI‑системи, такі як Looker, Power BI, Tableau